\documentclass[a4paper,11pt,]{article}
\usepackage{fancyvrb, color, graphbox, hyperref, amsmath, url}
%\usepackage{palatino}
\usepackage{pygments}

\usepackage[a4paper,text={16.5cm,25.2cm},centering]{geometry}
\usepackage{lipsum}
\usepackage[labelformat=simple,position=top]{subcaption}
% \renewcommand\thesubfigure{\alph{subfigure})}
\hypersetup{
  pdfauthor = {Srdjan Sarikas},
  pdftitle={Intracellular Receptors},
  colorlinks=TRUE,
  linkcolor=blue,
  citecolor=red,
  urlcolor=green
}
\usepackage{minted}
\newcommand{\py}[1]{{\mintinline{python}{#1}}}
\newcommand{\Regions}{{\texttt{Regions}}}

%\setlength{\parindent}{0pt}
%\setlength{\parskip}{1.2ex}

\title{Intracellular Receptors \\ {\small Materials and Methods: Analysis and Processing}}

\author{Srdjan}
\date{\today}

\begin{document}
\maketitle

%<<echo=False>>=
%import matplotlib.pyplot as plt
%import numpy as np
%import pandas as pd
%from islets import load_regions
%@

A typical experiment involving imaging of pancreatic slices in our lab concerns a single field of view (FOV)
showing 10s--100s of cells, in a recording of at least several, often dozens, of gigabytes.
Current tools (ImageJ, \dots) rely on loading the recording, or its part, into memory, for viewing, analysis, and processing.
It also requires laborious and long human engagement.
We have developed a set of interdependent tools to automatize as much as possible the pipeline. 
The crucial elements of our pipeline are the following:
\begin{itemize}
\item (Semi-)automatic detection of regions of interest (ROIs);
\item Transformation of ROI time traces into standard score ("z-score");
\item Quantification of the phenotype for each ROI in terms of the distribution of events of different durations.
\end{itemize}

Our toolset is inspired and relies on CaImAn \cite{giovannucci2019caiman}, a similar package developed for the purposes in neuroscience reseach.

\begin{figure}[h]
\centering
\includegraphics[width=\textwidth,trim=1.5cm 1.5cm 15mm 15mm,clip]{figures/pipeline.pdf}
\label{fig:pipeline}
\caption{
An illustration of our processing and analysis pipeline:
({\it i})  From a full movie, we calculate the mean (or other statistic) across all frames.
({\it ii}) We pass the mean image through a band-pass filter and define ROIs by detecting local peaks.
({\it iii}) We save ROIs with all the important information (time traces, ROI coordinates, movie statistics, recording frequency, pixel size, etc.).
({\it iv}) Traces contain features at very different time scales---with different timescales presumably important for different cell types. We collect them into separable events for analysis.
}
\end{figure}

\section{(Semi-)Automatic Detection of Regions of Interest}

Once imported, a recording is stored as a 3-dimensional ($T{\times}x{\times}y$) numpy array \cite{2020NumPy-Array}.
When the recording is stable, obtainaing a mean image, or any other statistic over frame, is rather trivial. 
In case there is horizontal movement, it can be corrected for by aligning the frames to a template. 
For this we use the functionality present in CaImAn \cite{giovannucci2019caiman}, except that high frequency recordings need to be rebinned to some moderate frequency (a few Hz), before correcting, in order to reduce the noise level. 
Once the translation offsets are obtained, we use them to correct the recording in original frequency.

To define regions of interest, we blurr the representative image by a kernel of the size we expect cells to be, 
and at the scale double of that.
The difference between these two images represents a band-pass filter of the original, where the local intensity variation are emphasized (fig.\ref{fig:regions}).
We then pass through all pixels where the value of the filtered image is positive (or larger than a small positive threshold), and for each pixel we search for a local peak in its vicinity. 
All the pixels that lead to the same local peak are then grouped into a single ROI.


\begin{figure}[t]
\centering
\begin{minipage}{.25\textwidth}
    \vskip 2mm
    {\fontfamily{phv}\selectfont A} \includegraphics[scale=.5,align=t,trim=25mm 15mm 130mm 72mm, clip]{figures/regions_ideal_real.pdf}\\
    {\fontfamily{phv}\selectfont C} \includegraphics[scale=.33,align=t,trim={-5mm 0 0 -10mm}]{figures/regions_stars.pdf}
\end{minipage}
\begin{minipage}{.6\textwidth}
    \vskip 7mm
    {\fontfamily{phv}\selectfont B} \includegraphics[scale=.5,align=t,trim=10mm 15mm 20mm 15mm, clip]{figures/regions_kernels.pdf}
\end{minipage}
\caption{
(A) Computationally created template $64{\times}64$ image with uniform spherical cells, and with Poisson noise added.
(B) Band-pass filtering of the "realistic" image from A, with different kernel sizes. The size of the kernel determines the approximate size of the ROIs obtained.
We in red we emphasize misidentified ROIs; the dots indicate the real locations of the cells.
(C) Each ROI is constructed by explicitly searching for a closest peak in intensity. A pixel can only be part of a single ROI.
\label{fig:regions}}
\end{figure}

Representative image can be a mean over all frames or any other statistic.
In addition our code supports standard deviation, mean and standard deviation of the first derivative of the movie, and a ``robust maximum'' of the movie.
As "robust maximum`` we define a very high percentile of the set absolute values of a set, essentially a value close to its maximum, by default it is 10th largest.
This statistic is sensitive to cells which fire extremely rarily during a recording, so that the mean of those pixels is negligible.
By default, we choose an average of the mean and high percentile as a representative image for band-pass filtering and ROI extraction.

% \subsection{\Regions~object}
% 
% The object \Regions~contains all the information about ROIs and the recording for which they are defined. 
% For example, say \py{regions} is a \Regions~object, \py{regions.Freq} is the frequency of the recording ROIs were inferred from, and \py{regions.time} is a numpy array of timepoints in steps of \py{regions.Freq}${}^{-1}$.
% \py{regions.statImages} is a dictionary of different statistics of the recording, \py{regions.mode} indicates which of them is used for filtering with kernel of size \py{regions.filterSize} to obtain the filtered image for ROI extraction (\py{regions.image}).
% 
% Information about the individual ROIs is stored in \py{regions.df} as pandas dataframe~\cite{reback2020pandas, mckinney-proc-scipy-2010}.
% Its most important columns are \py{pixels} and \py{peak} and \py{trace}. 
% ROIs are identified by the index in the dataframe. 
% For each row, \py{pixels} contain all the pixels that belong to the local peak, with coordinates \py{peak}. 
% Column \py{trace} contains arrays of an average light intensity over the appropriate pixels per frame.
% 
% The above mentioned \Regions's attributes and \py{Regions.df} columns are essential and are defined and saved when processing.
% 
% Other useful columns of the \Regions's dataframe include \py{size} --- number of pixels within ROI; \py{neighbors} --- list of other ROIs that are in immediate contact with the actual; \py{peakValue} --- value of the local peak in the band-filtered image.
% 
% Sometimes it is useful to define \py{movie} as a attribute of the \Regions object and make it point to the actual recording.
% 
% 
% Other useful columns of the \Regions's dataframe include \py{size} --- number of pixels within ROI; \py{neighbors} --- list of other ROIs that are in immediate contact with the actual; \py{peakValue} --- value of the local peak in the band-filtered image.

\bibliographystyle{plain}% E-Life prefers apa
\bibliography{matmet}


\end{document}
